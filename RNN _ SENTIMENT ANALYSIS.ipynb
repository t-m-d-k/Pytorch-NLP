{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e63a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaeaa19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sentiment.txt\") as f:\n",
    "    reviews = f.read()\n",
    "    \n",
    "data = pd.DataFrame([review.split('\\t') for review in reviews.split('\\n')])\n",
    "\n",
    "data.columns = ['Review','Sentiment']\n",
    "\n",
    "data = data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3ee18d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2579</th>\n",
       "      <td>Due to this happening on every call I was forc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>One of the few places in Phoenix that I would ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>Simply beautiful.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>Battery life is also great!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>That's right....the red velvet cake.....ohhh t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Review Sentiment\n",
       "2579  Due to this happening on every call I was forc...         0\n",
       "1766  One of the few places in Phoenix that I would ...         1\n",
       "949                                 Simply beautiful.           1\n",
       "2372                        Battery life is also great!         1\n",
       "1025  That's right....the red velvet cake.....ohhh t...         1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d52d342a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['due',\n",
       " 'to',\n",
       " 'this',\n",
       " 'happening',\n",
       " 'on',\n",
       " 'every',\n",
       " 'call',\n",
       " 'i',\n",
       " 'was',\n",
       " 'forced',\n",
       " 'to',\n",
       " 'stop',\n",
       " 'using',\n",
       " 'this',\n",
       " 'headset']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_words_reviews(data):\n",
    "    text = list(data['Review'].values)\n",
    "    clean_text = []\n",
    "    for t in text:\n",
    "        clean_text.append(t.translate(str.maketrans('', '', punctuation)).lower().rstrip())\n",
    "    tokenized = [word_tokenize(x) for x in clean_text]\n",
    "    all_text = []\n",
    "    for tokens in tokenized:\n",
    "        for t in tokens:\n",
    "            all_text.append(t)\n",
    "    return tokenized, set(all_text)\n",
    "\n",
    "reviews, vocab = split_words_reviews(data)\n",
    "\n",
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3270136b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'selfdiscovery',\n",
       " 2: 'life',\n",
       " 3: 'definitely',\n",
       " 4: 'palmtopcameracellphone',\n",
       " 5: 'indie',\n",
       " 6: 'suffering',\n",
       " 7: 'watered',\n",
       " 8: 'sympathetic',\n",
       " 9: 'r',\n",
       " 10: 'constructed',\n",
       " 11: 'aria',\n",
       " 12: 'meanings',\n",
       " 13: 'serve',\n",
       " 14: 'meats',\n",
       " 15: 'topvery',\n",
       " 16: 'chimplike',\n",
       " 17: 'disturbing',\n",
       " 18: 'tops',\n",
       " 19: 'tooth',\n",
       " 20: 'drinks',\n",
       " 21: 'earlier',\n",
       " 22: 'similar',\n",
       " 23: 'empty',\n",
       " 24: 'insulin',\n",
       " 25: 'yummy',\n",
       " 26: 'kabuki',\n",
       " 27: 'blanket',\n",
       " 28: 'lovely',\n",
       " 29: 'trythe',\n",
       " 30: 'monolog',\n",
       " 31: 'nuts',\n",
       " 32: 'sangria',\n",
       " 33: 'hanks',\n",
       " 34: 'favorite',\n",
       " 35: 'candace',\n",
       " 36: 'heaven',\n",
       " 37: 'experiences',\n",
       " 38: 'counter',\n",
       " 39: 'superficial',\n",
       " 40: 'boot',\n",
       " 41: 'buds',\n",
       " 42: 'shipping',\n",
       " 43: 'fanciful',\n",
       " 44: 'again',\n",
       " 45: 'bloodiest',\n",
       " 46: 'sells',\n",
       " 47: 'pancakes',\n",
       " 48: 'afraid',\n",
       " 49: 'seafood',\n",
       " 50: 'filmmostly',\n",
       " 51: 'rock',\n",
       " 52: 'taped',\n",
       " 53: 'across',\n",
       " 54: 'prompted',\n",
       " 55: 'loops',\n",
       " 56: 'unconditional',\n",
       " 57: 'setting',\n",
       " 58: 'joins',\n",
       " 59: 'workers',\n",
       " 60: 'cheekbones',\n",
       " 61: 'exquisite',\n",
       " 62: 'writer',\n",
       " 63: 'fall',\n",
       " 64: 'wobbly',\n",
       " 65: 'viewing',\n",
       " 66: 'downs',\n",
       " 67: 'cibo',\n",
       " 68: 'volcano',\n",
       " 69: 'jalapeno',\n",
       " 70: '8pm',\n",
       " 71: 'flawless',\n",
       " 72: 'pixel',\n",
       " 73: 'refund',\n",
       " 74: 'waited',\n",
       " 75: 'nevertheless',\n",
       " 76: 'scratched',\n",
       " 77: 'ebola',\n",
       " 78: 'gods',\n",
       " 79: 'james',\n",
       " 80: 'relaxing',\n",
       " 81: 'apart',\n",
       " 82: 'visually',\n",
       " 83: 'photograph',\n",
       " 84: 'colors',\n",
       " 85: 'guys',\n",
       " 86: 'count',\n",
       " 87: 'died',\n",
       " 88: 'regular',\n",
       " 89: 'accessory',\n",
       " 90: 'simmering',\n",
       " 91: 'charismatic',\n",
       " 92: 'functionality',\n",
       " 93: 'community',\n",
       " 94: 'cat',\n",
       " 95: 'adapters',\n",
       " 96: 'pneumatic',\n",
       " 97: 'time',\n",
       " 98: 'soooooo',\n",
       " 99: 'bluegreenscreen',\n",
       " 100: 'gotten',\n",
       " 101: 'headsets',\n",
       " 102: 'port',\n",
       " 103: 'refurb',\n",
       " 104: 'once',\n",
       " 105: 'nay',\n",
       " 106: 'satisfying',\n",
       " 107: 'screamy',\n",
       " 108: 'its',\n",
       " 109: 'material',\n",
       " 110: 'virgin',\n",
       " 111: 'rendering',\n",
       " 112: 'paced',\n",
       " 113: 'fry',\n",
       " 114: 'provided',\n",
       " 115: 'stand',\n",
       " 116: 'creaks',\n",
       " 117: 'skype',\n",
       " 118: 'add',\n",
       " 119: 'mess',\n",
       " 120: 'badly',\n",
       " 121: 'asked',\n",
       " 122: 'built',\n",
       " 123: 'stated',\n",
       " 124: 'shipment',\n",
       " 125: 'pumpkin',\n",
       " 126: 'searched',\n",
       " 127: 'spoil',\n",
       " 128: 'maintaining',\n",
       " 129: 'egg',\n",
       " 130: 'underneath',\n",
       " 131: 'cruel',\n",
       " 132: 'weekly',\n",
       " 133: 'financial',\n",
       " 134: 'checked',\n",
       " 135: 'repair',\n",
       " 136: 'cradle',\n",
       " 137: 'based',\n",
       " 138: 'crawl',\n",
       " 139: 'sci',\n",
       " 140: 'worthless',\n",
       " 141: 'uninspired',\n",
       " 142: 'decor',\n",
       " 143: '1948',\n",
       " 144: 'ever',\n",
       " 145: 'thomerson',\n",
       " 146: 'worry',\n",
       " 147: 'ms',\n",
       " 148: 'depicts',\n",
       " 149: 'skip',\n",
       " 150: 'characters',\n",
       " 151: 'monstrous',\n",
       " 152: 'feature',\n",
       " 153: 'delightful',\n",
       " 154: 'intoning',\n",
       " 155: 'ok',\n",
       " 156: 'controversy',\n",
       " 157: 'thrillerhorror',\n",
       " 158: 'such',\n",
       " 159: 'within',\n",
       " 160: 'humanity',\n",
       " 161: 'renowned',\n",
       " 162: 'akin',\n",
       " 163: 'person',\n",
       " 164: 'elegant',\n",
       " 165: 'evaluate',\n",
       " 166: 'verizon',\n",
       " 167: 'sum',\n",
       " 168: 'ipod',\n",
       " 169: 'remorse',\n",
       " 170: 'usefulness',\n",
       " 171: 'cannoli',\n",
       " 172: 'back',\n",
       " 173: 'timeless',\n",
       " 174: 'threepack',\n",
       " 175: 'yellow',\n",
       " 176: 'guess',\n",
       " 177: 'bamboo',\n",
       " 178: 'seeing',\n",
       " 179: 'educational',\n",
       " 180: 'pied',\n",
       " 181: 'handy',\n",
       " 182: 'movieits',\n",
       " 183: 'tongue',\n",
       " 184: 'having',\n",
       " 185: 'incendiary',\n",
       " 186: 'garbo',\n",
       " 187: 'anywhere',\n",
       " 188: 'frightening',\n",
       " 189: 'hadnt',\n",
       " 190: 'meat',\n",
       " 191: 'damn',\n",
       " 192: 'decide',\n",
       " 193: 'bug',\n",
       " 194: 'commands',\n",
       " 195: 'purchases',\n",
       " 196: 'deadpan',\n",
       " 197: 'takeout',\n",
       " 198: 'affected',\n",
       " 199: 'pasta',\n",
       " 200: 'operate',\n",
       " 201: 'signal',\n",
       " 202: 'coconut',\n",
       " 203: 'tvnever',\n",
       " 204: 'form',\n",
       " 205: 'ericsson',\n",
       " 206: 'fridays',\n",
       " 207: 'leopard',\n",
       " 208: 'sloppy',\n",
       " 209: 'started',\n",
       " 210: 'ought',\n",
       " 211: 'versatile',\n",
       " 212: 'yourself',\n",
       " 213: 'trilogy',\n",
       " 214: 'masterful',\n",
       " 215: 'rochonwas',\n",
       " 216: 'multiple',\n",
       " 217: 'marriage',\n",
       " 218: 'monica',\n",
       " 219: 'houses',\n",
       " 220: 'cumbersome',\n",
       " 221: 'classical',\n",
       " 222: 'bells',\n",
       " 223: 'portions',\n",
       " 224: 'jawbone',\n",
       " 225: 'unit',\n",
       " 226: 'mega',\n",
       " 227: 'corny',\n",
       " 228: 'credit',\n",
       " 229: 'excerpts',\n",
       " 230: 'rare',\n",
       " 231: 'disrespected',\n",
       " 232: 'battery',\n",
       " 233: 'timing',\n",
       " 234: 'selection',\n",
       " 235: 'periods',\n",
       " 236: '70s',\n",
       " 237: 'daughter',\n",
       " 238: 'sexobsessed',\n",
       " 239: 'courtroom',\n",
       " 240: 'rita',\n",
       " 241: 'woven',\n",
       " 242: 'groove',\n",
       " 243: 'crash',\n",
       " 244: 'hustons',\n",
       " 245: 'thought',\n",
       " 246: 'de',\n",
       " 247: 'recommended',\n",
       " 248: 'hearts',\n",
       " 249: 'marion',\n",
       " 250: 'yelps',\n",
       " 251: 'we',\n",
       " 252: 'tale',\n",
       " 253: 'imdb',\n",
       " 254: 'derivative',\n",
       " 255: 'ground',\n",
       " 256: 'anita',\n",
       " 257: 'humorous',\n",
       " 258: 'clothes',\n",
       " 259: 'current',\n",
       " 260: 'belt',\n",
       " 261: 'easily',\n",
       " 262: 'crappy',\n",
       " 263: 'upway',\n",
       " 264: 'unknown',\n",
       " 265: 'yucky',\n",
       " 266: 'cheeseburger',\n",
       " 267: 'element',\n",
       " 268: 'soggy',\n",
       " 269: 'moving',\n",
       " 270: 'liking',\n",
       " 271: 'underappreciated',\n",
       " 272: 'harris',\n",
       " 273: 'arts',\n",
       " 274: 'cassette',\n",
       " 275: 'choose',\n",
       " 276: 'two',\n",
       " 277: 'accidentally',\n",
       " 278: 'beeping',\n",
       " 279: 'good7',\n",
       " 280: 'haunt',\n",
       " 281: 'contained',\n",
       " 282: 'kill',\n",
       " 283: 'smooth',\n",
       " 284: 'carries',\n",
       " 285: 'counterfeit',\n",
       " 286: 'happier',\n",
       " 287: 'wifes',\n",
       " 288: 'theatres',\n",
       " 289: 'probably',\n",
       " 290: 'letting',\n",
       " 291: 'from',\n",
       " 292: 'mobile',\n",
       " 293: 'donut',\n",
       " 294: 'nc17',\n",
       " 295: 'conditions',\n",
       " 296: 'adaptation',\n",
       " 297: 'accomodate',\n",
       " 298: 'menu',\n",
       " 299: 'blacks',\n",
       " 300: 'carlys',\n",
       " 301: 'replace',\n",
       " 302: 'endall',\n",
       " 303: 'subplot',\n",
       " 304: 'prepared',\n",
       " 305: 'keeping',\n",
       " 306: 'trip',\n",
       " 307: 'soundwise',\n",
       " 308: 'phonemy',\n",
       " 309: 'yum',\n",
       " 310: 'calligraphy',\n",
       " 311: 'wired',\n",
       " 312: 'damian',\n",
       " 313: 'additional',\n",
       " 314: 'camelback',\n",
       " 315: 'achievement',\n",
       " 316: 'steamboat',\n",
       " 317: 'directions',\n",
       " 318: 'stinks',\n",
       " 319: 'gloriously',\n",
       " 320: 'verbatim',\n",
       " 321: 'saw',\n",
       " 322: 'austens',\n",
       " 323: 'relative',\n",
       " 324: 'ages',\n",
       " 325: 'sophisticated',\n",
       " 326: 'flash',\n",
       " 327: 'provokes',\n",
       " 328: 'jessica',\n",
       " 329: 'drunk',\n",
       " 330: 'spiffy',\n",
       " 331: 'onedimensional',\n",
       " 332: 'washing',\n",
       " 333: 'reaching',\n",
       " 334: 'elaborately',\n",
       " 335: 'hummus',\n",
       " 336: 'line',\n",
       " 337: 'following',\n",
       " 338: 'sync',\n",
       " 339: 'correctly',\n",
       " 340: 'greatness',\n",
       " 341: 'jaclyn',\n",
       " 342: 'jimmy',\n",
       " 343: 'elses',\n",
       " 344: 'bela',\n",
       " 345: 'mere',\n",
       " 346: 'lucio',\n",
       " 347: 'shine',\n",
       " 348: 'hi',\n",
       " 349: 'contact',\n",
       " 350: 'bisque',\n",
       " 351: 'ownerchef',\n",
       " 352: 'chipolte',\n",
       " 353: 'noircrimedrama',\n",
       " 354: 'knock',\n",
       " 355: 'rings',\n",
       " 356: 'remake',\n",
       " 357: 'koteasjack',\n",
       " 358: 'organizational',\n",
       " 359: 'loop',\n",
       " 360: 'docking',\n",
       " 361: 'indian',\n",
       " 362: 'coastal',\n",
       " 363: 'buffalo',\n",
       " 364: 'sheer',\n",
       " 365: 'intentions',\n",
       " 366: 'uneasy',\n",
       " 367: 'clock',\n",
       " 368: 'warranty',\n",
       " 369: 'emily',\n",
       " 370: 'sliding',\n",
       " 371: 'spice',\n",
       " 372: 'establishment',\n",
       " 373: 'crisp',\n",
       " 374: 'interview',\n",
       " 375: 'heard',\n",
       " 376: 'subplots',\n",
       " 377: 'block',\n",
       " 378: 'dr',\n",
       " 379: 'sand',\n",
       " 380: 'because',\n",
       " 381: 'create',\n",
       " 382: 'spoilers',\n",
       " 383: 'grace',\n",
       " 384: 'nobu',\n",
       " 385: 'exercise',\n",
       " 386: 'watsons',\n",
       " 387: 'without',\n",
       " 388: 'maintain',\n",
       " 389: 'size',\n",
       " 390: 'creamy',\n",
       " 391: 'could',\n",
       " 392: 'spicy',\n",
       " 393: 'awards',\n",
       " 394: 'wouldve',\n",
       " 395: 'accessible',\n",
       " 396: 'emotionally',\n",
       " 397: 'connoisseur',\n",
       " 398: 'z',\n",
       " 399: 'indulgent',\n",
       " 400: 'physical',\n",
       " 401: 'sensitivities',\n",
       " 402: 'chosen',\n",
       " 403: 'filmmaker',\n",
       " 404: 'slices',\n",
       " 405: 'fellow',\n",
       " 406: 'transformed',\n",
       " 407: 'shape',\n",
       " 408: 'crowd',\n",
       " 409: 'delicioso',\n",
       " 410: 'all',\n",
       " 411: 'company',\n",
       " 412: 'phonesmp3',\n",
       " 413: 'ended',\n",
       " 414: 'easy',\n",
       " 415: 'breeze',\n",
       " 416: 'atmosphere1',\n",
       " 417: 'ta',\n",
       " 418: 'sat',\n",
       " 419: 'recognizes',\n",
       " 420: 'cape',\n",
       " 421: 'oysters',\n",
       " 422: 'industrial',\n",
       " 423: 'usual',\n",
       " 424: 'rated',\n",
       " 425: 'mickey',\n",
       " 426: 'sack',\n",
       " 427: 'sex',\n",
       " 428: 'dinner',\n",
       " 429: 'head',\n",
       " 430: 'hoping',\n",
       " 431: 'jamie',\n",
       " 432: 'thinly',\n",
       " 433: 'm',\n",
       " 434: 'seat',\n",
       " 435: 'native',\n",
       " 436: 'wear',\n",
       " 437: 'grey',\n",
       " 438: 'caterpillar',\n",
       " 439: 'shield',\n",
       " 440: 'hip',\n",
       " 441: 'humor',\n",
       " 442: 'exciting',\n",
       " 443: 'seeen',\n",
       " 444: 'keira',\n",
       " 445: 'diaper',\n",
       " 446: 'sour',\n",
       " 447: 'everyones',\n",
       " 448: 'rolled',\n",
       " 449: 'slightly',\n",
       " 450: 'gratitude',\n",
       " 451: 'friends',\n",
       " 452: 'gooodd',\n",
       " 453: 'tool',\n",
       " 454: '8525',\n",
       " 455: 'emotions',\n",
       " 456: 'sanyo',\n",
       " 457: 'madison',\n",
       " 458: 'swords',\n",
       " 459: 'charger',\n",
       " 460: 'selfsacrifice',\n",
       " 461: 'gristle',\n",
       " 462: 'noise',\n",
       " 463: 'chip',\n",
       " 464: 'fan',\n",
       " 465: 'external',\n",
       " 466: 'satisfied',\n",
       " 467: 'thinking',\n",
       " 468: 'idiot',\n",
       " 469: 'randomly',\n",
       " 470: 'comedic',\n",
       " 471: 'screened',\n",
       " 472: 'today',\n",
       " 473: 'cocktails',\n",
       " 474: 'body',\n",
       " 475: 'father',\n",
       " 476: 'anne',\n",
       " 477: 'diabetic',\n",
       " 478: 'scot',\n",
       " 479: 'piano',\n",
       " 480: 'climbing',\n",
       " 481: 'characterisation',\n",
       " 482: 'angeles',\n",
       " 483: 'southwest',\n",
       " 484: 'itdefinitely',\n",
       " 485: 'celebration',\n",
       " 486: 'sending',\n",
       " 487: 'curve',\n",
       " 488: 'ben',\n",
       " 489: 'decisions',\n",
       " 490: '1947',\n",
       " 491: 'genre',\n",
       " 492: 'greek',\n",
       " 493: 'd807',\n",
       " 494: 'latifas',\n",
       " 495: 'lemon',\n",
       " 496: 'theft',\n",
       " 497: 'pineapple',\n",
       " 498: 'bird',\n",
       " 499: 'magazine',\n",
       " 500: 'absolutel',\n",
       " 501: 'accommodations',\n",
       " 502: 'point',\n",
       " 503: 'veteran',\n",
       " 504: 'philippa',\n",
       " 505: 'utter',\n",
       " 506: '20th',\n",
       " 507: 'hate',\n",
       " 508: 'boasts',\n",
       " 509: 'sashimi',\n",
       " 510: 'late',\n",
       " 511: 'america',\n",
       " 512: 'package',\n",
       " 513: 'drawing',\n",
       " 514: 'hey',\n",
       " 515: 'stuff',\n",
       " 516: 'decipher',\n",
       " 517: 'noises',\n",
       " 518: 'excessively',\n",
       " 519: 'bob',\n",
       " 520: 'politics',\n",
       " 521: 'cole',\n",
       " 522: 'plot',\n",
       " 523: 'sharp',\n",
       " 524: 'outshining',\n",
       " 525: 'scenery',\n",
       " 526: 'every',\n",
       " 527: 'silly',\n",
       " 528: 'min',\n",
       " 529: 'cheaply',\n",
       " 530: 'carol',\n",
       " 531: 'despite',\n",
       " 532: 'leaves',\n",
       " 533: 'detailed',\n",
       " 534: 'barren',\n",
       " 535: 'fisted',\n",
       " 536: 'room',\n",
       " 537: 'depth',\n",
       " 538: 'shouting',\n",
       " 539: 'abysmal',\n",
       " 540: 'soon',\n",
       " 541: 'z500a',\n",
       " 542: 'phone',\n",
       " 543: 'wallet',\n",
       " 544: 'quicker',\n",
       " 545: 'give',\n",
       " 546: 'end',\n",
       " 547: 'blew',\n",
       " 548: 'subgenre',\n",
       " 549: 'cartoon',\n",
       " 550: 'occasional',\n",
       " 551: 'outit',\n",
       " 552: 'cellphones',\n",
       " 553: 'rinse',\n",
       " 554: 'threw',\n",
       " 555: 'spices',\n",
       " 556: '1998',\n",
       " 557: 'bertolucci',\n",
       " 558: 'sincere',\n",
       " 559: 'spacek',\n",
       " 560: 'fat',\n",
       " 561: 'design',\n",
       " 562: 'hardly',\n",
       " 563: 'comparablypriced',\n",
       " 564: 'stale',\n",
       " 565: 'howe',\n",
       " 566: 'pancake',\n",
       " 567: 'paradise',\n",
       " 568: 'stupid',\n",
       " 569: 'masculine',\n",
       " 570: 'sam',\n",
       " 571: 'universe',\n",
       " 572: '3o',\n",
       " 573: 'precisely',\n",
       " 574: 'original',\n",
       " 575: 'overwhelm',\n",
       " 576: 'howeverthe',\n",
       " 577: 'blackberry',\n",
       " 578: 'outlets',\n",
       " 579: 'wind',\n",
       " 580: 'owners',\n",
       " 581: 'pushed',\n",
       " 582: 'cords',\n",
       " 583: 'along',\n",
       " 584: 'eaten',\n",
       " 585: 'hero',\n",
       " 586: 'artless',\n",
       " 587: 'done',\n",
       " 588: 'causing',\n",
       " 589: 'exteriors',\n",
       " 590: 'impossible',\n",
       " 591: 'ask',\n",
       " 592: 'son',\n",
       " 593: 'scripting',\n",
       " 594: 'together',\n",
       " 595: 'solid',\n",
       " 596: 'showcasing',\n",
       " 597: 'dissapointing',\n",
       " 598: 'stunning',\n",
       " 599: 'competent',\n",
       " 600: 'threshold',\n",
       " 601: 'issues',\n",
       " 602: 'jabra350',\n",
       " 603: 'flakes',\n",
       " 604: 'value',\n",
       " 605: 'tone',\n",
       " 606: 'flynn',\n",
       " 607: 'open',\n",
       " 608: 'compete',\n",
       " 609: 'excrutiatingly',\n",
       " 610: 'blist',\n",
       " 611: 'removing',\n",
       " 612: 'repertory',\n",
       " 613: 'paul',\n",
       " 614: 'creates',\n",
       " 615: 'theater',\n",
       " 616: 'nevsky',\n",
       " 617: 'distracting',\n",
       " 618: 'ruthless',\n",
       " 619: 'sprouts',\n",
       " 620: 'latin',\n",
       " 621: 'ians',\n",
       " 622: 'priced',\n",
       " 623: 'discount',\n",
       " 624: 'cover',\n",
       " 625: 'curry',\n",
       " 626: 'sequels',\n",
       " 627: 'underlines',\n",
       " 628: 'idiotsavant',\n",
       " 629: 'three',\n",
       " 630: 'b',\n",
       " 631: 'trashy',\n",
       " 632: 'guests',\n",
       " 633: 'compared',\n",
       " 634: 'sobaditsgood',\n",
       " 635: 'lets',\n",
       " 636: 'snider',\n",
       " 637: 'ri',\n",
       " 638: 'martin',\n",
       " 639: 'high',\n",
       " 640: 'navigate',\n",
       " 641: 'refused',\n",
       " 642: 'need',\n",
       " 643: 'hundred',\n",
       " 644: 'backdrop',\n",
       " 645: 'tom',\n",
       " 646: 'presents',\n",
       " 647: 'aesthetically',\n",
       " 648: 'meals',\n",
       " 649: 'finale',\n",
       " 650: 'unfolds',\n",
       " 651: 'purcashed',\n",
       " 652: 'taco',\n",
       " 653: 'regrettably',\n",
       " 654: 'pastry',\n",
       " 655: 'embassy',\n",
       " 656: 'told',\n",
       " 657: 'connection',\n",
       " 658: 'underacting',\n",
       " 659: 'minutes',\n",
       " 660: 'included',\n",
       " 661: 'amp',\n",
       " 662: 'nasty',\n",
       " 663: 'discovering',\n",
       " 664: 'strangers',\n",
       " 665: 'ladies',\n",
       " 666: 'chickenwith',\n",
       " 667: 'fiancã©',\n",
       " 668: 'smoothly',\n",
       " 669: 'earpieces',\n",
       " 670: 'greasy',\n",
       " 671: 'using',\n",
       " 672: 'seating',\n",
       " 673: 'mp3',\n",
       " 674: 'join',\n",
       " 675: 'bipolarity',\n",
       " 676: 'locations',\n",
       " 677: 'sorely',\n",
       " 678: 'deep',\n",
       " 679: 'nonetheless',\n",
       " 680: 'macarons',\n",
       " 681: 'covered',\n",
       " 682: 'excessive',\n",
       " 683: 'wonderfully',\n",
       " 684: 'connery',\n",
       " 685: 'undoubtedly',\n",
       " 686: 'excellent',\n",
       " 687: 'visited',\n",
       " 688: 'ugly',\n",
       " 689: 'radiant',\n",
       " 690: 'expert',\n",
       " 691: 'job',\n",
       " 692: 'leaf',\n",
       " 693: 'move',\n",
       " 694: 'neighborhood',\n",
       " 695: 'tardis',\n",
       " 696: 'traffic',\n",
       " 697: 'lestat',\n",
       " 698: 'relatively',\n",
       " 699: 'outperform',\n",
       " 700: 'letdown',\n",
       " 701: 'stuffed',\n",
       " 702: 'legs',\n",
       " 703: 'prevents',\n",
       " 704: 'breaking',\n",
       " 705: 'down',\n",
       " 706: 'zombiez',\n",
       " 707: 'afternoon',\n",
       " 708: 'arepas',\n",
       " 709: 'supposed',\n",
       " 710: 'etcits',\n",
       " 711: 'basically',\n",
       " 712: 'modest',\n",
       " 713: 'rendition',\n",
       " 714: 'passion',\n",
       " 715: 'bus',\n",
       " 716: 'fruit',\n",
       " 717: 'resolution',\n",
       " 718: 'designed',\n",
       " 719: 'puppets',\n",
       " 720: 'rpger',\n",
       " 721: 'imperial',\n",
       " 722: 'delete',\n",
       " 723: 'brat',\n",
       " 724: 'understanding',\n",
       " 725: 'irons',\n",
       " 726: 'welsh',\n",
       " 727: 'plants',\n",
       " 728: 'showed',\n",
       " 729: 'itfriendly',\n",
       " 730: 'difference',\n",
       " 731: 'needs',\n",
       " 732: 'episodes',\n",
       " 733: 'must',\n",
       " 734: 'let',\n",
       " 735: 'musicincluding',\n",
       " 736: 'sits',\n",
       " 737: 'disbelief',\n",
       " 738: 'trash',\n",
       " 739: 'activesync',\n",
       " 740: 'right',\n",
       " 741: 'dumbest',\n",
       " 742: 'pats',\n",
       " 743: 'itself',\n",
       " 744: '1980s',\n",
       " 745: 'elegantly',\n",
       " 746: 'requested',\n",
       " 747: 'overt',\n",
       " 748: 'sugary',\n",
       " 749: 'tape',\n",
       " 750: 'girlfriendboyfriend',\n",
       " 751: 'breakfastlunch',\n",
       " 752: 'robotic',\n",
       " 753: 'string',\n",
       " 754: 'dipping',\n",
       " 755: 'match',\n",
       " 756: 'inhouse',\n",
       " 757: 'nerves',\n",
       " 758: 'effective',\n",
       " 759: 'outrageously',\n",
       " 760: 'random',\n",
       " 761: 'wrongfirst',\n",
       " 762: 'quite',\n",
       " 763: 'button',\n",
       " 764: 'an',\n",
       " 765: 'puff',\n",
       " 766: 'fare',\n",
       " 767: 'nuns',\n",
       " 768: 'animation',\n",
       " 769: 'sub',\n",
       " 770: 'narrative',\n",
       " 771: 'technically',\n",
       " 772: 'enter',\n",
       " 773: 'date',\n",
       " 774: 'fire',\n",
       " 775: 'places',\n",
       " 776: 'photo',\n",
       " 777: 'outta',\n",
       " 778: 'wed',\n",
       " 779: 'shawarrrrrrma',\n",
       " 780: 'barney',\n",
       " 781: 'array',\n",
       " 782: 'continually',\n",
       " 783: 'require',\n",
       " 784: 's11',\n",
       " 785: 'edge',\n",
       " 786: 'side',\n",
       " 787: 'rate',\n",
       " 788: 'track',\n",
       " 789: 'ussr',\n",
       " 790: 'indeed',\n",
       " 791: 'patio',\n",
       " 792: 'role',\n",
       " 793: 'mouths',\n",
       " 794: 'receiving',\n",
       " 795: 'rough',\n",
       " 796: 'purchased',\n",
       " 797: 'save',\n",
       " 798: 'management',\n",
       " 799: 'mail',\n",
       " 800: 'staying',\n",
       " 801: 'dialing',\n",
       " 802: 'gets',\n",
       " 803: 'schrader',\n",
       " 804: 'iphone',\n",
       " 805: 'whom',\n",
       " 806: 'roths',\n",
       " 807: 'far',\n",
       " 808: 'surprises',\n",
       " 809: 'decent',\n",
       " 810: 'joy',\n",
       " 811: 'bt50',\n",
       " 812: 'otherwise',\n",
       " 813: 'plater',\n",
       " 814: 'shops',\n",
       " 815: 'crashed',\n",
       " 816: 'else',\n",
       " 817: 'interesting',\n",
       " 818: 'duck',\n",
       " 819: 'beginning',\n",
       " 820: 'promise',\n",
       " 821: 'borders',\n",
       " 822: 'stoic',\n",
       " 823: 'vampire',\n",
       " 824: 'faceplates',\n",
       " 825: 'pain',\n",
       " 826: 'broke',\n",
       " 827: 'ford',\n",
       " 828: 'feels',\n",
       " 829: 'brainsucking',\n",
       " 830: 'vehicle',\n",
       " 831: 'stateoftheart',\n",
       " 832: 'worstannoying',\n",
       " 833: 'crocs',\n",
       " 834: 'detailing',\n",
       " 835: 'reverse',\n",
       " 836: 'wonderful',\n",
       " 837: 'boiled',\n",
       " 838: 'insult',\n",
       " 839: 'actress',\n",
       " 840: 'made',\n",
       " 841: 'taylor',\n",
       " 842: 'sobering',\n",
       " 843: 'carbs',\n",
       " 844: 'atrocity',\n",
       " 845: 'locks',\n",
       " 846: 'evening',\n",
       " 847: 'lacks',\n",
       " 848: 'mclaglen',\n",
       " 849: 'lyrics',\n",
       " 850: 'us',\n",
       " 851: 'fundamental',\n",
       " 852: 'whenever',\n",
       " 853: 'rubbish',\n",
       " 854: 'diverse',\n",
       " 855: 'noir',\n",
       " 856: 'ethic',\n",
       " 857: 'cbr',\n",
       " 858: 'overcome',\n",
       " 859: 'douchey',\n",
       " 860: 'hands',\n",
       " 861: 'pop',\n",
       " 862: 'strident',\n",
       " 863: 'dozens',\n",
       " 864: '13',\n",
       " 865: 'find',\n",
       " 866: 'vinaigrette',\n",
       " 867: 'peach',\n",
       " 868: 'onethis',\n",
       " 869: 'resume',\n",
       " 870: 'able',\n",
       " 871: 'broken',\n",
       " 872: 'according',\n",
       " 873: 'imagine',\n",
       " 874: 'gloveseverything',\n",
       " 875: 'wise',\n",
       " 876: 'editing',\n",
       " 877: 'dosent',\n",
       " 878: 's',\n",
       " 879: 'credits',\n",
       " 880: 'generic',\n",
       " 881: 'phantasm',\n",
       " 882: 'owning',\n",
       " 883: 'sounded',\n",
       " 884: 'going',\n",
       " 885: 'tap',\n",
       " 886: 'spinach',\n",
       " 887: 'versus',\n",
       " 888: 'relationship',\n",
       " 889: 'jutland',\n",
       " 890: 'specialand',\n",
       " 891: 'luvs',\n",
       " 892: 'band',\n",
       " 893: 'managementoh',\n",
       " 894: 'air',\n",
       " 895: 'welldone',\n",
       " 896: 'spends',\n",
       " 897: 'jean',\n",
       " 898: 'garlic',\n",
       " 899: 'pulled',\n",
       " 900: 'labute',\n",
       " 901: 'either',\n",
       " 902: 'filet',\n",
       " 903: 'costs',\n",
       " 904: 'equipment',\n",
       " 905: 'approval',\n",
       " 906: 'cut',\n",
       " 907: 'gyros',\n",
       " 908: 'viewers',\n",
       " 909: 'never',\n",
       " 910: 'computer',\n",
       " 911: 'someone',\n",
       " 912: 'fell',\n",
       " 913: 'id',\n",
       " 914: 'options',\n",
       " 915: 'disappointment',\n",
       " 916: 'capability',\n",
       " 917: 'decay',\n",
       " 918: 'bucks',\n",
       " 919: 'something',\n",
       " 920: 'attacked',\n",
       " 921: 'v3c',\n",
       " 922: 'yawn',\n",
       " 923: 'pg13',\n",
       " 924: 'runs',\n",
       " 925: 'exploit',\n",
       " 926: 'beef',\n",
       " 927: 'guilt',\n",
       " 928: 'protection',\n",
       " 929: 'hernandez',\n",
       " 930: 'purã©ed',\n",
       " 931: 'jerky',\n",
       " 932: 'movie',\n",
       " 933: 'nakedbilly',\n",
       " 934: 'known',\n",
       " 935: 'noodles',\n",
       " 936: 'struggle',\n",
       " 937: 'was',\n",
       " 938: 'cinematographyif',\n",
       " 939: 'stranger',\n",
       " 940: 'texas',\n",
       " 941: 'whose',\n",
       " 942: 'angus',\n",
       " 943: 'performing',\n",
       " 944: 'itmy',\n",
       " 945: 'disliked',\n",
       " 946: 'morning',\n",
       " 947: 'is',\n",
       " 948: 'connect',\n",
       " 949: 'corn',\n",
       " 950: 'iq',\n",
       " 951: 'ue',\n",
       " 952: 'exceeding',\n",
       " 953: 'planned',\n",
       " 954: 'huston',\n",
       " 955: 'loosely',\n",
       " 956: 'hardest',\n",
       " 957: 'week',\n",
       " 958: 'indescribably',\n",
       " 959: 'cast',\n",
       " 960: 'villain',\n",
       " 961: 'speaker',\n",
       " 962: 'wifetobe',\n",
       " 963: 'european',\n",
       " 964: 'so',\n",
       " 965: 'chow',\n",
       " 966: 'puzzlesolving',\n",
       " 967: 'apple',\n",
       " 968: 'inexpensive',\n",
       " 969: 'tots',\n",
       " 970: 'edinburgh',\n",
       " 971: 'propaganda',\n",
       " 972: 'transfer',\n",
       " 973: 'abovepretty',\n",
       " 974: 'couldnt',\n",
       " 975: 'shocking',\n",
       " 976: 'lesson',\n",
       " 977: 'values',\n",
       " 978: 'sauce',\n",
       " 979: 'wouldnt',\n",
       " 980: 'upgrading',\n",
       " 981: 'marine',\n",
       " 982: 'pleased',\n",
       " 983: 'pucks',\n",
       " 984: 'cutest',\n",
       " 985: 'thru',\n",
       " 986: 'tech',\n",
       " 987: 'happy',\n",
       " 988: 'wayyy',\n",
       " 989: 'unsatisfactory',\n",
       " 990: 'strawberry',\n",
       " 991: 'connections',\n",
       " 992: 'predictable',\n",
       " 993: 'actions',\n",
       " 994: 'finds',\n",
       " 995: 'whenscamp',\n",
       " 996: 'sequence',\n",
       " 997: 'taste',\n",
       " 998: 'notice',\n",
       " 999: 'appreciate',\n",
       " 1000: 'reccomendation',\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dictionaries(words):\n",
    "    word_to_int_dict = {w:i+1 for i, w in enumerate(words)}\n",
    "    int_to_word_dict = {i:w for w, i in word_to_int_dict.items()}\n",
    "    return word_to_int_dict, int_to_word_dict\n",
    "\n",
    "word_to_int_dict, int_to_word_dict = create_dictionaries(vocab)\n",
    "\n",
    "int_to_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f33273f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_to_int_dict.json', 'w') as fp:\n",
    "    json.dump(word_to_int_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0610587e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', 'due', 'to', 'this', 'happening', 'on', 'every', 'call', 'i',\n",
       "       'was', 'forced', 'to', 'stop', 'using', 'this', 'headset'],\n",
       "      dtype='<U33')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_text(tokenized_reviews, seq_length):\n",
    "    \n",
    "    reviews = []\n",
    "    \n",
    "    for review in tokenized_reviews:\n",
    "        if len(review) >= seq_length:\n",
    "            reviews.append(review[:seq_length])\n",
    "        else:\n",
    "            reviews.append(['']*(seq_length-len(review)) + review)\n",
    "        \n",
    "    return np.array(reviews)\n",
    "\n",
    "padded_sentences = pad_text(reviews, seq_length = 50)\n",
    "\n",
    "padded_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "183d14cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_word_dict[0] = ''\n",
    "word_to_int_dict[''] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1355cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0, 2419, 1729, 2801, 5156, 4817,  526, 4298, 2774,  937,\n",
       "       2849, 1729, 4482,  671, 2801, 1078])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sentences = np.array([[word_to_int_dict[word] for word in review] for review in padded_sentences])\n",
    "\n",
    "encoded_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a10f6614",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0.8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_vocab = n_vocab  \n",
    "        self.n_layers = n_layers \n",
    "        self.n_hidden = n_hidden \n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                          \n",
    "        embedded_words = self.embedding(input_words)\n",
    "        lstm_out, h = self.lstm(embedded_words) \n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden)\n",
    "        fc_out = self.fc(lstm_out)                  \n",
    "        sigmoid_out = self.sigmoid(fc_out)              \n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  \n",
    "        \n",
    "        sigmoid_last = sigmoid_out[:, -1]\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):\n",
    "        \n",
    "        device = \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56c5f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(word_to_int_dict)\n",
    "n_embed = 50\n",
    "n_hidden = 100\n",
    "n_output = 1\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a57f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([int(x) for x in data['Sentiment'].values])\n",
    "\n",
    "train_ratio = 0.8\n",
    "valid_ratio = (1 - train_ratio)/2\n",
    "\n",
    "total = len(encoded_sentences)\n",
    "train_cutoff = int(total * train_ratio)\n",
    "valid_cutoff = int(total * (1 - valid_ratio))\n",
    "\n",
    "train_x, train_y = torch.Tensor(encoded_sentences[:train_cutoff]).long(), torch.Tensor(labels[:train_cutoff]).long()\n",
    "valid_x, valid_y = torch.Tensor(encoded_sentences[train_cutoff : valid_cutoff]).long(), torch.Tensor(labels[train_cutoff : valid_cutoff]).long()\n",
    "test_x, test_y = torch.Tensor(encoded_sentences[valid_cutoff:]).long(), torch.Tensor(labels[valid_cutoff:])\n",
    "\n",
    "train_data = TensorDataset(train_x, train_y)\n",
    "valid_data = TensorDataset(valid_x, valid_y)\n",
    "test_data = TensorDataset(test_x, test_y)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4dc05beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 2400\n",
    "step = 0\n",
    "n_epochs = 33\n",
    "clip = 5  \n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f64278f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50d0deff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DK\\AppData\\Local\\Temp\\ipykernel_4232\\1344997538.py:10: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(net.parameters(), clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/33 Step: 2400 Training Loss: 0.3792 Validation Loss: 0.5718\n",
      "Epoch: 2/33 Step: 4800 Training Loss: 1.4280 Validation Loss: 0.5825\n",
      "Epoch: 3/33 Step: 7200 Training Loss: 0.0001 Validation Loss: 0.7038\n",
      "Epoch: 4/33 Step: 9600 Training Loss: 0.0124 Validation Loss: 0.8224\n",
      "Epoch: 5/33 Step: 12000 Training Loss: 0.0001 Validation Loss: 1.1294\n",
      "Epoch: 6/33 Step: 14400 Training Loss: 0.0418 Validation Loss: 1.1879\n",
      "Epoch: 7/33 Step: 16800 Training Loss: 0.0067 Validation Loss: 1.1236\n",
      "Epoch: 8/33 Step: 19200 Training Loss: 0.0000 Validation Loss: 1.4580\n",
      "Epoch: 9/33 Step: 21600 Training Loss: 0.0001 Validation Loss: 1.6247\n",
      "Epoch: 10/33 Step: 24000 Training Loss: 0.0000 Validation Loss: 1.6294\n",
      "Epoch: 11/33 Step: 26400 Training Loss: 0.0001 Validation Loss: 1.8337\n",
      "Epoch: 12/33 Step: 28800 Training Loss: 0.0001 Validation Loss: 1.8131\n",
      "Epoch: 13/33 Step: 31200 Training Loss: 0.0000 Validation Loss: 2.0953\n",
      "Epoch: 14/33 Step: 33600 Training Loss: 0.0000 Validation Loss: 2.1164\n",
      "Epoch: 15/33 Step: 36000 Training Loss: 0.0000 Validation Loss: 4.0803\n",
      "Epoch: 16/33 Step: 38400 Training Loss: 0.0000 Validation Loss: 5.3400\n",
      "Epoch: 17/33 Step: 40800 Training Loss: 0.0000 Validation Loss: 5.1544\n",
      "Epoch: 18/33 Step: 43200 Training Loss: 0.0000 Validation Loss: 4.4685\n",
      "Epoch: 19/33 Step: 45600 Training Loss: 0.0000 Validation Loss: 3.9188\n",
      "Epoch: 20/33 Step: 48000 Training Loss: 0.0000 Validation Loss: 3.9250\n",
      "Epoch: 21/33 Step: 50400 Training Loss: 0.0004 Validation Loss: 4.9068\n",
      "Epoch: 22/33 Step: 52800 Training Loss: 0.0000 Validation Loss: 5.5287\n",
      "Epoch: 23/33 Step: 55200 Training Loss: 0.0000 Validation Loss: 9.5572\n",
      "Epoch: 24/33 Step: 57600 Training Loss: 0.0000 Validation Loss: 6.6349\n",
      "Epoch: 25/33 Step: 60000 Training Loss: 0.0000 Validation Loss: 5.4234\n",
      "Epoch: 26/33 Step: 62400 Training Loss: 0.0000 Validation Loss: 5.5888\n",
      "Epoch: 27/33 Step: 64800 Training Loss: 0.0000 Validation Loss: 8.7233\n",
      "Epoch: 28/33 Step: 67200 Training Loss: 0.0000 Validation Loss: 6.2212\n",
      "Epoch: 29/33 Step: 69600 Training Loss: 0.0000 Validation Loss: 4.8871\n",
      "Epoch: 30/33 Step: 72000 Training Loss: 0.0000 Validation Loss: 7.2526\n",
      "Epoch: 31/33 Step: 74400 Training Loss: 0.0000 Validation Loss: 8.1429\n",
      "Epoch: 32/33 Step: 76800 Training Loss: 0.0000 Validation Loss: 6.3183\n",
      "Epoch: 33/33 Step: 79200 Training Loss: 0.0000 Validation Loss: 7.6040\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        step += 1  \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs)\n",
    "        loss = criterion(output, labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "\n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                       \n",
    "                v_output, v_h = net(v_inputs)\n",
    "                v_loss = criterion(v_output, v_labels.float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34edfca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6ab9027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)\n",
    "net.load_state_dict(torch.load('model.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf7ff610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(review):\n",
    "    review = review.translate(str.maketrans('', '', punctuation)).lower().rstrip()\n",
    "    tokenized = word_tokenize(review)\n",
    "    if len(tokenized) >= 50:\n",
    "        review = tokenized[:50]\n",
    "    else:\n",
    "        review= ['0']*(50-len(tokenized)) + tokenized\n",
    "    \n",
    "    final = []\n",
    "    \n",
    "    for token in review:\n",
    "        try:\n",
    "            final.append(word_to_int_dict[token])\n",
    "            \n",
    "        except:\n",
    "            final.append(word_to_int_dict[''])\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "903e3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(review):\n",
    "    net.eval()\n",
    "    words = np.array([preprocess_review(review)])\n",
    "    padded_words = torch.from_numpy(words)\n",
    "    pred_loader = DataLoader(padded_words, batch_size = 1, shuffle = True)\n",
    "    for x in pred_loader:\n",
    "        output = net(x)[0].item()\n",
    "    \n",
    "    msg = \"This is a positive review.\" if output >= 0.5 else \"This is a negative review.\"\n",
    "    print(msg)\n",
    "    print('Prediction = ' + str(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9ad08c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a negative review.\n",
      "Prediction = 5.213006026849598e-09\n"
     ]
    }
   ],
   "source": [
    "predict(\"the song is bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4317c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
